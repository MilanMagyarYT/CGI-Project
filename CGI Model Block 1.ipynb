{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "926afc48-b14e-4a89-a579-e659a2fe7802",
   "metadata": {},
   "source": [
    "Load of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1bf250d-a1a4-4aee-b6f2-5b4273c83053",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the paths to your datasets\n",
    "sensor_data_path = r'telemetry.csv'\n",
    "failure_data_path = r'failures.csv'\n",
    "\n",
    "# Load the sensor data\n",
    "sensor_data = pd.read_csv(sensor_data_path)\n",
    "\n",
    "# Load the failure data\n",
    "failure_data = pd.read_csv(failure_data_path)\n",
    "\n",
    "# Convert datetime columns to datetime type for both datasets\n",
    "sensor_data['datetime'] = pd.to_datetime(sensor_data['datetime'])\n",
    "failure_data['datetime'] = pd.to_datetime(failure_data['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34f846a9-b106-4b96-abce-9e6a55c4fef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5561/1283988652.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  sensor_data[f'{column}_mean'] = sensor_data.groupby('machineID')[column].transform(lambda x: x.rolling(window_size).mean())\n",
      "/tmp/ipykernel_5561/1283988652.py:12: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  sensor_data[f'{column}_std'] = sensor_data.groupby('machineID')[column].transform(lambda x: x.rolling(window_size).std())\n",
      "/tmp/ipykernel_5561/1283988652.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  sensor_data[f'{column}_mean'] = sensor_data.groupby('machineID')[column].transform(lambda x: x.rolling(window_size).mean())\n",
      "/tmp/ipykernel_5561/1283988652.py:12: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  sensor_data[f'{column}_std'] = sensor_data.groupby('machineID')[column].transform(lambda x: x.rolling(window_size).std())\n",
      "/tmp/ipykernel_5561/1283988652.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  sensor_data[f'{column}_mean'] = sensor_data.groupby('machineID')[column].transform(lambda x: x.rolling(window_size).mean())\n",
      "/tmp/ipykernel_5561/1283988652.py:12: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  sensor_data[f'{column}_std'] = sensor_data.groupby('machineID')[column].transform(lambda x: x.rolling(window_size).std())\n",
      "/tmp/ipykernel_5561/1283988652.py:11: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  sensor_data[f'{column}_mean'] = sensor_data.groupby('machineID')[column].transform(lambda x: x.rolling(window_size).mean())\n",
      "/tmp/ipykernel_5561/1283988652.py:12: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  sensor_data[f'{column}_std'] = sensor_data.groupby('machineID')[column].transform(lambda x: x.rolling(window_size).std())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     machineID        volt   volt_mean   volt_std      rotate  \\\n",
      "datetime                                                                        \n",
      "2015-01-01 06:00:00          1  176.217853  176.217853  14.916611  418.504078   \n",
      "2015-01-01 06:00:00         53  183.084582  183.084582  14.916611  420.980061   \n",
      "2015-01-01 06:00:00         99  168.596133  168.596133  14.916611  384.747105   \n",
      "2015-01-01 06:00:00         12  171.404215  171.404215  14.916611  576.923563   \n",
      "2015-01-01 06:00:00          6  136.878588  136.878588  14.916611  492.088420   \n",
      "...                        ...         ...         ...        ...         ...   \n",
      "2015-01-01 06:00:00         30  179.301175  179.301175  14.916611  441.944165   \n",
      "2015-01-01 06:00:00          3  185.482043  185.482043  14.916611  461.211137   \n",
      "2015-01-01 06:00:00         32  161.194392  161.194392  14.916611  367.942046   \n",
      "2015-01-01 06:00:00         70  162.628154  162.628154  14.916611  429.216322   \n",
      "2015-01-01 06:00:00         49  179.950557  179.950557  14.916611  451.390724   \n",
      "\n",
      "                     rotate_mean  rotate_std    pressure  pressure_mean  \\\n",
      "datetime                                                                  \n",
      "2015-01-01 06:00:00   418.504078    49.94625  113.077935     113.077935   \n",
      "2015-01-01 06:00:00   420.980061    49.94625  109.235805     109.235805   \n",
      "2015-01-01 06:00:00   384.747105    49.94625  110.921131     110.921131   \n",
      "2015-01-01 06:00:00   576.923563    49.94625   97.145400      97.145400   \n",
      "2015-01-01 06:00:00   492.088420    49.94625  149.003582     149.003582   \n",
      "...                          ...         ...         ...            ...   \n",
      "2015-01-01 06:00:00   441.944165    49.94625   92.010346      92.010346   \n",
      "2015-01-01 06:00:00   461.211137    49.94625   87.453199      87.453199   \n",
      "2015-01-01 06:00:00   367.942046    49.94625  127.774288     127.774288   \n",
      "2015-01-01 06:00:00   429.216322    49.94625   98.680784      98.680784   \n",
      "2015-01-01 06:00:00   451.390724    49.94625  111.233725     111.233725   \n",
      "\n",
      "                     pressure_std  vibration  vibration_mean  vibration_std  \n",
      "datetime                                                                     \n",
      "2015-01-01 06:00:00     10.046747  45.087686       45.087686        5.00215  \n",
      "2015-01-01 06:00:00     10.046747  45.737760       45.737760        5.00215  \n",
      "2015-01-01 06:00:00     10.046747  41.944692       41.944692        5.00215  \n",
      "2015-01-01 06:00:00     10.046747  47.725909       47.725909        5.00215  \n",
      "2015-01-01 06:00:00     10.046747  22.973289       22.973289        5.00215  \n",
      "...                           ...        ...             ...            ...  \n",
      "2015-01-01 06:00:00     10.046747  40.219164       40.219164        5.00215  \n",
      "2015-01-01 06:00:00     10.046747  28.216864       28.216864        5.00215  \n",
      "2015-01-01 06:00:00     10.046747  42.210941       42.210941        5.00215  \n",
      "2015-01-01 06:00:00     10.046747  43.482879       43.482879        5.00215  \n",
      "2015-01-01 06:00:00     10.046747  42.263020       42.263020        5.00215  \n",
      "\n",
      "[100 rows x 13 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5561/1283988652.py:20: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  sensor_data[column].fillna(avg_std, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "sensor_data.set_index('datetime', inplace=True)\n",
    "\n",
    "# Sort by datetime to ensure rolling windows work correctly\n",
    "sensor_data.sort_index(inplace=True)\n",
    "\n",
    "# Choose a window size, e.g., 24 hours\n",
    "window_size = '24H'\n",
    "\n",
    "# Create rolling features for each sensor\n",
    "for column in ['volt', 'rotate', 'pressure', 'vibration']:\n",
    "    sensor_data[f'{column}_mean'] = sensor_data.groupby('machineID')[column].transform(lambda x: x.rolling(window_size).mean())\n",
    "    sensor_data[f'{column}_std'] = sensor_data.groupby('machineID')[column].transform(lambda x: x.rolling(window_size).std())\n",
    "\n",
    "# For each standard deviation column, fill NaN values with the average of available standard deviations for that sensor\n",
    "for column in ['volt_std', 'rotate_std', 'pressure_std', 'vibration_std']:\n",
    "    # Calculate the average standard deviation for the column, excluding NaN values\n",
    "    avg_std = sensor_data[column].mean()\n",
    "    \n",
    "    # Fill NaN values in the standard deviation column with this average standard deviation\n",
    "    sensor_data[column].fillna(avg_std, inplace=True)\n",
    "    \n",
    "print(sensor_data[['machineID', 'volt', 'volt_mean', 'volt_std', 'rotate', 'rotate_mean', 'rotate_std', 'pressure', 'pressure_mean', 'pressure_std', 'vibration', 'vibration_mean', 'vibration_std']].head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c938ca74-38e7-4410-b631-7dcad98700fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a column for labels\n",
    "sensor_data['failure_within_48h'] = 0\n",
    "\n",
    "# For each failure, mark the preceding 48 hours as positive examples\n",
    "for index, row in failure_data.iterrows():\n",
    "    start_time = row['datetime'] - pd.Timedelta(hours=48)\n",
    "    end_time = row['datetime']\n",
    "    machine_id = row['machineID']\n",
    "    \n",
    "    sensor_data.loc[(sensor_data.index > start_time) & (sensor_data.index <= end_time) & (sensor_data['machineID'] == machine_id), 'failure_within_48h'] = 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fa268f1-b8e7-43bd-8cd9-c1265b53be34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data based on a date. For example, use the last 20% of the dates as the test set.\n",
    "split_date = sensor_data.index.max() - pd.Timedelta(days=365 * 0.2)  # Adjust based on your dataset's date range\n",
    "\n",
    "train_data = sensor_data[sensor_data.index < split_date]\n",
    "test_data = sensor_data[sensor_data.index >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd861a0-1a2f-4b6a-9de8-9bf2348cff5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Features and target variable\n",
    "X_train = train_data.drop(['failure_within_48h', 'machineID'], axis=1)\n",
    "y_train = train_data['failure_within_48h']\n",
    "X_test = test_data.drop(['failure_within_48h', 'machineID'], axis=1)\n",
    "y_test = test_data['failure_within_48h']\n",
    "\n",
    "# Apply SMOTE to balance the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Since we've applied SMOTE, convert the resampled datasets to DMatrix for XGBoost\n",
    "dtrain_resampled = xgb.DMatrix(X_train_resampled, label=y_train_resampled)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a337cd2e-0a2b-46bd-81d0-ea6dd316d133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttest-auc:0.90512\n",
      "[1]\ttest-auc:0.90958\n",
      "[2]\ttest-auc:0.91607\n",
      "[3]\ttest-auc:0.92073\n",
      "[4]\ttest-auc:0.92363\n",
      "[5]\ttest-auc:0.92402\n",
      "[6]\ttest-auc:0.92461\n",
      "[7]\ttest-auc:0.92539\n",
      "[8]\ttest-auc:0.92606\n",
      "[9]\ttest-auc:0.92578\n",
      "[10]\ttest-auc:0.92623\n",
      "[11]\ttest-auc:0.92571\n",
      "[12]\ttest-auc:0.92558\n",
      "[13]\ttest-auc:0.92573\n",
      "[14]\ttest-auc:0.92534\n",
      "[15]\ttest-auc:0.92475\n",
      "[16]\ttest-auc:0.92543\n",
      "[17]\ttest-auc:0.92512\n",
      "[18]\ttest-auc:0.92496\n",
      "[19]\ttest-auc:0.92507\n",
      "[20]\ttest-auc:0.92473\n",
      "Accuracy: 0.8736052481460354\n",
      "Precision: 0.20445103857566765\n",
      "Recall: 0.886031184696994\n",
      "F1 Score: 0.33223832916428075\n",
      "Confusion Matrix:\n",
      "[[147631  21448]\n",
      " [   709   5512]]\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost parameters\n",
    "params = {\n",
    "    'max_depth': 6,  # Depth of each tree\n",
    "    'eta': 0.3,  # Learning rate\n",
    "    'objective': 'binary:logistic',  # Binary classification\n",
    "    'eval_metric': 'auc',  # Evaluation metric\n",
    "    'nthread': 4  # Number of cores to use\n",
    "}\n",
    "\n",
    "num_boost_round = 100\n",
    "\n",
    "# Train the model on the resampled (balanced) dataset\n",
    "bst_resampled = xgb.train(params, dtrain_resampled, num_boost_round, evals=[(dtest, 'test')], early_stopping_rounds=10)\n",
    "\n",
    "# Predict the probabilities of failure\n",
    "y_pred_proba_resampled = bst_resampled.predict(dtest)\n",
    "\n",
    "# Convert probabilities to binary predictions using a threshold, e.g., 0.5\n",
    "y_pred_resampled = [1 if x > 0.5 else 0 for x in y_pred_proba_resampled]\n",
    "\n",
    "# Calculate and print accuracy, precision, recall, and F1 score\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_resampled))\n",
    "print(\"Precision:\", precision_score(y_test, y_pred_resampled))\n",
    "print(\"Recall:\", recall_score(y_test, y_pred_resampled))\n",
    "print(\"F1 Score:\", f1_score(y_test, y_pred_resampled))\n",
    "\n",
    "# Generate and print the confusion matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_resampled)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd1ea013-b7e7-4607-817c-8f82587e68d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold for Target Precision ~80.0% and Recall between 30.0% and 35.0%: 0.9102581739425659\n",
      "Metrics with 0.80 Threshold:\n",
      "Accuracy: 0.9432\n",
      "Precision: 0.2758\n",
      "Recall: 0.3691\n",
      "F1 Score: 0.3157\n",
      "Confusion Matrix with 0.80 Threshold:\n",
      "[[163049   6030]\n",
      " [  3925   2296]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "import numpy as np\n",
    "\n",
    "# Generate the precision-recall curve\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_proba_resampled)\n",
    "\n",
    "# Concatenate a threshold of 0 at the beginning (useful for plotting)\n",
    "thresholds = np.insert(thresholds, 0, 0)\n",
    "\n",
    "# Calculate the differences to target precision and recall\n",
    "target_precision = 0.8\n",
    "target_recall_range = (0.3, 0.35)\n",
    "precision_diff = np.abs(precision - target_precision)\n",
    "recall_in_range = (recall >= target_recall_range[0]) & (recall <= target_recall_range[1])\n",
    "\n",
    "# Find the index of the threshold that meets the criteria\n",
    "# Prioritize meeting the precision criterion and recall being in the specified range\n",
    "valid_indices = np.where(recall_in_range)[0]  # Indices where recall is in the desired range\n",
    "closest_precision_idx = valid_indices[np.argmin(precision_diff[valid_indices])]  # Closest to target precision within recall range\n",
    "\n",
    "# Best threshold based on criteria\n",
    "best_threshold = thresholds[closest_precision_idx]\n",
    "print(f\"Best Threshold for Target Precision ~{target_precision*100}% and Recall between {target_recall_range[0]*100}% and {target_recall_range[1]*100}%: {best_threshold}\")\n",
    "\n",
    "# Apply this threshold to convert probabilities to binary predictions\n",
    "# Precision is 0.28 and Recall is 0.30 for 0.9102 threshold => y_pred_adjusted = [1 if prob >= best_threshold else 0 for prob in y_pred_proba_resampled]\n",
    "y_pred_threshold_080 = [1 if prob >= 0.90 else 0 for prob in y_pred_proba_resampled]\n",
    "\n",
    "# Evaluate the model with the new threshold\n",
    "accuracy_080 = accuracy_score(y_test, y_pred_threshold_080)\n",
    "precision_080 = precision_score(y_test, y_pred_threshold_080)\n",
    "recall_080 = recall_score(y_test, y_pred_threshold_080)\n",
    "f1_score_080 = f1_score(y_test, y_pred_threshold_080)\n",
    "\n",
    "# Print the updated metrics\n",
    "print(f\"Metrics with 0.80 Threshold:\")\n",
    "print(f\"Accuracy: {accuracy_080:.4f}\")\n",
    "print(f\"Precision: {precision_080:.4f}\")\n",
    "print(f\"Recall: {recall_080:.4f}\")\n",
    "print(f\"F1 Score: {f1_score_080:.4f}\")\n",
    "\n",
    "# Generate and print the confusion matrix\n",
    "conf_matrix_080 = confusion_matrix(y_test, y_pred_threshold_080)\n",
    "print(\"Confusion Matrix with 0.80 Threshold:\")\n",
    "print(conf_matrix_080)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802306b-3422-40fd-87f5-a3d2417ee65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
